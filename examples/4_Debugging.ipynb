{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c86094-2c1f-4f44-810c-28d0734b6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is designed to teach you some things to do if training seems to fail. \n",
    "    In general, it is good practice to use some of these tricks before training even begins, but sometimes stuff simply goes wrong anyway.\n",
    "\n",
    "1) Typically you want to train a model many times with the full dataset. \n",
    "    During debugging, we attempt to isolate the problem. In this case, that means overfitting to a single minibatch.\n",
    "\n",
    "2) We use the structures and code generated during previous steps to make the experience uniform, but change the training loop for a simpler overfitting loop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86251fca-9a74-4efb-86ad-34734692589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.unet import Configuration\n",
    "\n",
    "# load configuration information\n",
    "configuration = Configuration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6496e11-b9c5-4480-9944-9575a2edf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.dataloader import load_data_wrapper\n",
    "from src.losses import select_loss_fnc\n",
    "from src.model import model_loader_wrapper\n",
    "from src.viz import eval_unlabelled_images, plot_loss_and_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f57e8-6585-4531-abfb-b61927756437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = load_data_wrapper(**configuration)\n",
    "model = model_loader_wrapper(**configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DON'T FORGET\n",
    "# - you can use device=\"cpu\" to run everything on cpu for easier debugging - don't forget to reproduce the model and data on these devices\n",
    "# - you can wrap the whole for-loop in a `with torch.autograd.detect_anomaly():` context to get debugging feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96680d97-125b-4300-8268-4869c2c7c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start by setting up the optimization parameters\n",
    "loss_fnc = select_loss_fnc(**configuration)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=100)\n",
    "\n",
    "data, target = next(iter(dataloaders['train']))\n",
    "data = data.to(configuration.device)\n",
    "target = target.to(configuration.device)\n",
    "\n",
    "losses = []  # keep track of our losses\n",
    "\n",
    "for i in range(100):\n",
    "    # print(i)  # sometimes it can be good to keep track of how far along we are\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # calculate the model's output based on the data and sigmoid to transform to range 0-1\n",
    "    output = model(data)\n",
    "    output = output.sigmoid()\n",
    "\n",
    "    # calculcate loss\n",
    "    loss = loss_fnc(output, output)\n",
    "\n",
    "    # track the loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # calculate gradients and update weights\n",
    "    loss.backward\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e2453",
   "metadata": {},
   "source": [
    "# Questions/exercises:\n",
    "\n",
    "1. As mentioned, the functions in optimizer.py have a lot of reporting information that are not essential for the training of a model.\n",
    "    Try to make a copy of the two functions in optimizer.py and remove all the reporting information and make the code as simple as possible.\n",
    "    Can you train your model with these new functions?\n",
    "\n",
    "2. At each epoch of the optimization, the model goes through the training dataset followed by the validation dataset.\n",
    "    As previously stated, the purpose of the validation dataset is to help find the best model, how should the validation dataset help with this? (How would you incorporate that into your optimization function?)\n",
    "3. How does the validation dataset differ from the test dataset?\n",
    "4. How come the validation loss is lower than the training loss?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
