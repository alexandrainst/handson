{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c86094-2c1f-4f44-810c-28d0734b6890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this second example, we will take a closer look at the dataloader.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this second example, we will take a closer look at the dataloader.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86251fca-9a74-4efb-86ad-34734692589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.efficient_net_b2_pretrained import Configuration\n",
    "# load configuration information\n",
    "configuration = Configuration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6496e11-b9c5-4480-9944-9575a2edf66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_394674/813029754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0mOur\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcats\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdogs\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "What you see here is fairly standard in pytorch.\n",
    "We have created a custom dataset class, inheriting from torch.utils.data.Dataset.\n",
    "The dataset knows the amount of samples in the dataset and have some iterator responsible for fetching new samples as needed.\n",
    "A dataloader is a wrapper around a dataset such that we can fetch not just one example at a time, but a whole batch of them.\n",
    "The dataloader also provides a bunch of performance improvements that we won't get into now, but which you can read more on in the advanced section later.\n",
    "\"\"\"\n",
    "\n",
    "def load_data_wrapper(path_input_train,path_input_val,path_input_test,n_train,n_val,n_test,batch_size_train,batch_size_val,batch_size_test, device,**kwargs):\n",
    "    \"\"\"\n",
    "    A wrapper that allows us to handle different datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    dataloaders = load_cat_dog_data(path_input_train,path_input_val,path_input_test,n_train,n_val,n_test,batch_size_train,batch_size_val,batch_size_test, device)\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "def load_cat_dog_data(path_input_train,path_input_val,path_input_test,n_train,n_val,n_test,batch_size_train,batch_size_val,batch_size_test, device):\n",
    "    dataloaders = {}\n",
    "\n",
    "    dataset_train = ImageDataset(path_input_train,n_train,device=device)\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(dataset_train,shuffle=True, batch_size=batch_size_train,drop_last=True)\n",
    "\n",
    "    dataset_val = ImageDataset(path_input_val,n_val,device=device)\n",
    "    dataloaders['val'] = torch.utils.data.DataLoader(dataset_val,shuffle=False, batch_size=batch_size_val)\n",
    "\n",
    "    dataset_test = ImageDataset(path_input_test,n_test,device=device,training_data=False)\n",
    "    dataloaders['test'] = torch.utils.data.DataLoader(dataset_test,shuffle=False, batch_size=batch_size_test)\n",
    "    return dataloaders\n",
    "\n",
    "def eval_transform():\n",
    "    \"\"\"\n",
    "    A transform that crops an image to size of 256x256 converts it to a torch tensor\n",
    "    and normalizes the data according to the imagenet dataset.\n",
    "    \"\"\"\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.CenterCrop(256),  # crops the image in the center\n",
    "        torchvision.transforms.ToTensor(), # This permutes the dimensions of the image such that they are ordered the way neural networks usually works with them and then converts their datatype into floats or doubles.\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Our custom dataset used for cats and dogs images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path,n_samples=-1,extension='jpg',transform=eval_transform(),training_data=True,device='cpu'):\n",
    "        self.path = path\n",
    "        self.n_samples = n_samples\n",
    "        self.transform = transform\n",
    "        self.training_data = training_data\n",
    "        self.device = device\n",
    "        search_param = os.path.join(path,f\"*.{extension}\")\n",
    "        files = glob.glob(search_param)\n",
    "        assert len(files)>0, f\"Searching for images files in {search_param} gave zero hits. Make sure you already downloaded the dataset using /data/download_cats_and_dogs.py\"\n",
    "        shuffle(files) # Remember to shuffle the pictures before you extract a subset of them!\n",
    "        self.files = files[:n_samples]\n",
    "        self.label_names = ['cat', 'dog']\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + ')'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    "        name = file.split('/')[-1]\n",
    "        if self.training_data:\n",
    "            if \"cat\" in name:\n",
    "                label = 0\n",
    "            elif \"dog\" in name:\n",
    "                label = 1\n",
    "            else:\n",
    "                raise ValueError(f\"{file} does not contain dog or cat label.\")\n",
    "        else:\n",
    "            label = -1\n",
    "\n",
    "        # load image\n",
    "        image = Image.open(file)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = image.to(device=self.device)\n",
    "        label = torch.tensor(label,device=self.device,dtype=torch.int64)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f57e8-6585-4531-abfb-b61927756437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = load_data_wrapper(**configuration)\n",
    "dataloader_train = dataloaders['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e2453",
   "metadata": {},
   "source": [
    "# A few questions to consider:\n",
    "\n",
    "1. How do we get a sample out of a dataloader?\n",
    "2. How do we know how many samples are in a dataloader?\n",
    "3. Why are we returning 3 different dataloaders?\n",
    "4. What happens if an image has both the name cat and dog in its filename?, what should happen?\n",
    "\n",
    "# Some more advanced questions:\n",
    "\n",
    "5. When we extract a random subset of our data, (as we currently do in the dataloader), we will likely not end up with a completely balanced subset, is this a problem? how could you fix this?\n",
    "6. Our current setup handles images of different sizes by using a transformation that crops them to the centermost 256x256 pixels. When could this be a problem, and what are some alternatives?\n",
    "7. Currently, we do not have any data augmentation. What kind of data augmentation might we do on images?\n",
    "\n",
    "## Further considerations \n",
    "In our example we used accuracy as a measure, but for unbalanced datasets this might not be a good measure.\n",
    "   (imagine an unbalanced dataset with 99 cat images and 1 dog image.)\n",
    "   by always predicting cat we will reach 99% accuracy on such a dataset.\n",
    "\n",
    "   Alternatively our dataset could be balanced but the importance of different predictions could be skewed.\n",
    "   For cancer screening for instance a false negative might mean that you miss that someone has cancer, whereas a false positive means that an unnecessary person gets additional testing.\n",
    "\n",
    "   So depending on the problem and the importance of true positives, true negatives, false positives and false negatives, we have various measures that are usefull.\n",
    "   For more information search google for: \"f1-score, recall, precision, accuracy\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
